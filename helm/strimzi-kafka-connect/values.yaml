---

# https://github.com/strimzi/strimzi-kafka-operator/blob/main/helm-charts/helm3/strimzi-kafka-operator/values.yaml
# https://github.com/strimzi/strimzi-kafka-operator/blob/main/documentation/modules/operators/con-configuring-cluster-operator.adoc
clusterOperator:
  enabled: false

  replicas: 1

  leaderElection:
    enable: false

  image:
    imagePullPolicy: IfNotPresent

  logLevel: WARN

  fullReconciliationIntervalMs: 60000

  generatePodDisruptionBudget: true

  resources:
    limits:
      memory: 384Mi
      cpu: 500m
    requests:
      memory: 384Mi
      cpu: 50m

  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                strimzi.io/kind: cluster-operator
            topologyKey: kubernetes.io/hostname

commonLabels:
  app.kubernetes.io/version: '{{ .Chart.Version }}'
  app.kubernetes.io/app-version: '{{ .Chart.AppVersion }}'
  app.kubernetes.io/name: kafka-connect
  app.kubernetes.io/instance: '{{  tpl .Values.strimziConfig.connectClusterName .  }}'
  app.kubernetes.io/part-of: 'strimzi-{{  tpl .Values.strimziConfig.connectClusterName .  }}'

commonAnnotations: {}

strimziConfig:
  labels:
    app.kubernetes.io/version: '{{ .Chart.Version }}'
    app.kubernetes.io/app-version: '{{ .Chart.AppVersion }}'
## The following labels are predefined and cannot be changed for resources created by strimzi operator:
#   app.kubernetes.io/name: kafka-connect
#   app.kubernetes.io/instance: '{{ .Values.strimziConfig.connectClusterName }}'
#   app.kubernetes.io/part-of: 'strimzi-{{ .Values.strimziConfig.connectClusterName }}'
#   app.kubernetes.io/managed-by: strimzi-cluster-operator
#   strimzi.io/kind: KafkaConnect
#   strimzi.io/name: '{{ .Values.strimziConfig.connectClusterName }}-connect'
#   strimzi.io/cluster: '{{ .Values.strimziConfig.connectClusterName }}'
#   strimzi.io/component-type: kafka-connect

  annotations: {}

  version: '{{ .Chart.AppVersion }}'

  connectClusterName: '{{ .Release.Name }}-kafka-connect'

  useConnectorResources: true

  bootstrapServers: ''

  schemaRegistry: ''

  authentication: {}

  tls: {}

  rack: {}

  clientRackInitImage: ''

  build: {}

  # https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaJmxOptions-reference
  jmxOptions: {}

  # https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaConnectTemplate-reference
  # Not considered: podDisruptionBudget, podSet, pod, connectContainer
  advancedConfig: {}

  metricsConfig:
    enabled: false
    debeziumRules: false
    strimziRules: true
    type: jmxPrometheusExporter
    valueFrom:
      configMapKeyRef:
        name: '{{ .Release.Name }}-prometheus-metrics-config'
        key: metricsConfig

  # https://strimzi.io/docs/operators/latest/configuring.html#property-kafka-connect-logging-reference
  loggingConfig:
    type: inline
    loggers:
      connect.root.logger.level: WARN # [INFO, ERROR, WARN, TRACE, DEBUG, FATAL, OFF]
## Plaintext logging is used by default (provided for informational purposes)
#      log4j.appender.CONSOLE: org.apache.log4j.ConsoleAppender
#      log4j.appender.CONSOLE.layout: org.apache.log4j.PatternLayout
## However, you can specify JSON, which may be necessary in log collection systems
#      log4j.appender.CONSOLE.layout: net.logstash.log4j.JSONEventLayoutV1

  tracingConfig:
    enabled: false
    parameters:
      ## opentelemetry
#      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-opentelemetry-collector.observability.svc.cluster.local:4817 # Gateway case
#      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
#      OTEL_SERVICE_NAME: '{{ tpl .Values.strimziConfig.connectClusterName . }}-otel'
#      OTEL_PROPAGATORS: tracecontext # [tracecontext, b3multi]
#      OTEL_TRACES_EXPORTER: otlp
#      OTEL_TRACES_SAMPLER: always_on
#      OTEL_METRICS_EXPORTER: none
      ## jaeger
#      OTEL_EXPORTER_JAEGER_ENDPOINT: http://otel-opentelemetry-collector.observability.svc.cluster.local:6831 # Gateway case
#      OTEL_EXPORTER_JAEGER_PROTOCOL: udp/thrift.compact
#      OTEL_SERVICE_NAME: '{{ tpl .Values.strimziConfig.connectClusterName . }}-jaeger'
#      OTEL_TRACES_EXPORTER: jaeger
      ## zipkin
#      OTEL_EXPORTER_ZIPKIN_ENDPOINT: http://otel-opentelemetry-collector.observability.svc.cluster.local:9411/api/v2/spans
#      OTEL_SERVICE_NAME: '{{ tpl .Values.strimziConfig.connectClusterName . }}-zipkin'
#      OTEL_TRACES_EXPORTER: zipkin

deploymentConfig:
  replicaCount: 1

  # https://strimzi.io/docs/operators/latest/configuring.html#con-common-configuration-images-reference
  image:
    registry: ghcr.io
    repository: dkadetov/strimzi-kafka-connect
    tag: latest

  imagePullSecrets: []

  # https://strimzi.io/docs/operators/latest/configuring.html#con-common-configuration-jvm-reference
  # https://strimzi.io/docs/operators/latest/configuring.html#type-JvmOptions-reference
  jvmOptions:
    gcLoggingEnabled: false
    javaSystemProperties: []
    -XX: {}
    -Xms: ''
    -Xmx: ''

  resources:
    limits:
      cpu: 500m
      memory: 1536Mi
    requests:
      cpu: 100m
      memory: 1536Mi

  # https://strimzi.io/docs/operators/latest/configuring.html#type-Probe-reference
  readinessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1

  # https://strimzi.io/docs/operators/latest/configuring.html#type-Probe-reference
  livenessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 10
    successThreshold: 1

  podSecurityContext:
    runAsUser: 1001
    fsGroup: 0

  connectSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    runAsNonRoot: true
    runAsUser: 1001
    seccompProfile:
      type: RuntimeDefault

  priorityClassName: ''

  terminationGracePeriodSeconds: 30

  # If you want to use custom pod disruption budget, set clusterOperator.generatePodDisruptionBudget to false.
  podDisruptionBudget: {}

  topologySpreadConstraints: []

  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                strimzi.io/cluster: '{{ tpl .Values.strimziConfig.connectClusterName . }}'
            topologyKey: kubernetes.io/hostname

  tolerations: []

  hostAliases: []

  schedulerName: ''

  enableServiceLinks: false

  tmpDirSizeLimit: ''

  extraEnv:
    - name: CLASSPATH
      value: /opt/kafka/plugins/avro-converter/*
    - name: K8S_APP_NAME
      value: '{{ .Chart.Name }}'
    - name: K8S_APP_VERSION
      value: '{{ .Chart.Version }}'
    - name: K8S_APP_INSTANCE
      value: '{{ .Release.Name }}'
    - name: DEBEZIUM_PG_USER
      valueFrom:
        secretKeyRef:
          name: '{{ tpl .Values.externalConfig.debeziumConfig.secretName $ }}'
          key: '{{ tpl .Values.externalConfig.debeziumConfig.postgresUsernameKey $ }}'
    - name: DEBEZIUM_PG_PASS
      valueFrom:
        secretKeyRef:
          name: '{{ tpl .Values.externalConfig.debeziumConfig.secretName $ }}'
          key: '{{ tpl .Values.externalConfig.debeziumConfig.postgresPasswordKey $ }}'

  # https://strimzi.io/docs/operators/latest/configuring.html#con-common-configuration-volumes-reference
  # https://strimzi.io/docs/operators/latest/configuring.html#type-AdditionalVolume-reference
  extraVolumes:
    - name: external-configuration
      configMap:
        name: '{{ tpl .Values.externalConfig.configMap.name $ }}'
        defaultMode: 365
        optional: true

  extraVolumeMounts:
    - name: external-configuration
      mountPath: '{{ tpl .Values.externalConfig.configMap.mountPath $ }}'

# https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaConnectSpec-reference
connectConfig:
  plugin.discovery: HYBRID_WARN # [ONLY_SCAN, HYBRID_WARN, HYBRID_FAIL, SERVICE_LOAD]
  group.id: '{{ .Release.Name }}-connect-cluster'
  connector.client.config.override.policy: All # [None, All]
  topic.creation.enable: true
  offset.flush.interval.ms: 30000 # default: 60000
  offset.storage.topic: '{{ .Release.Name }}-connect-cluster-offsets'
  offset.storage.replication.factor: 3
  offset.storage.partitions: 3 # >= deploymentConfig.replicaCount
  config.storage.topic: '{{ .Release.Name }}-connect-cluster-configs'
  config.storage.replication.factor: 3
  config.storage.partitions: 1 # usually one partition is enough
  status.storage.topic: '{{ .Release.Name }}-connect-cluster-status'
  status.storage.replication.factor: 3
  status.storage.partitions: 3 # >= deploymentConfig.replicaCount
  key.converter: io.confluent.connect.avro.AvroConverter
  key.converter.schema.registry.url: '{{ tpl .Values.strimziConfig.schemaRegistry $ }}'
  key.converter.schemas.enable: false
  value.converter: io.confluent.connect.avro.AvroConverter
  value.converter.schema.registry.url: '{{ tpl .Values.strimziConfig.schemaRegistry $ }}'
  value.converter.schemas.enable: false
  auto.register.schemas: true
  config.providers: env, file, directory, secrets, configmaps
  config.providers.env.class: org.apache.kafka.common.config.provider.EnvVarConfigProvider
  config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider
  config.providers.directory.class: org.apache.kafka.common.config.provider.DirectoryConfigProvider
  config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider
  config.providers.configmaps.class: io.strimzi.kafka.KubernetesConfigMapConfigProvider

# https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaConnector-reference
connectorConfig:
  common:
    spec:
      state: running # [running, paused, stopped]
      autoRestart: true
      maxRestarts: '' # empty == indefinitely
      tasksMax: 1
      listOffsets:
        enabled: false
        configMapName: ''
      alterOffsets:
        enabled: false
        configMapName: ''
    config:
      topic.creation.default.replication.factor: 3
      topic.creation.default.partitions: 6
      topic.creation.default.cleanup.policy: compact
      topic.creation.default.compression.type: producer # [uncompressed, zstd, lz4, snappy, gzip, producer]

      producer.override.compression.type: lz4
      topic.creation.groups: compacted, deleted, dev

      topic.creation.compacted.replication.factor: 3
      topic.creation.compacted.partitions: 6
      topic.creation.compacted.cleanup.policy: compact
      topic.creation.compacted.compression.type: producer

      topic.creation.deleted.replication.factor: 3
      topic.creation.deleted.partitions: 6
      topic.creation.deleted.cleanup.policy: delete
      topic.creation.deleted.compression.type: producer

      topic.creation.dev.replication.factor: 3
      topic.creation.dev.partitions: 4
      topic.creation.dev.cleanup.policy: compact
      topic.creation.dev.compression.type: producer
  debezium:
    spec:
      class: io.debezium.connector.postgresql.PostgresConnector
    instances: []
    config:
      topic.creation.heartbeat.replication.factor: 3
      topic.creation.heartbeat.partitions: 1
      topic.creation.heartbeat.cleanup.policy: delete
      topic.creation.heartbeat.compression.type: lz4
      topic.creation.heartbeat.retention.ms: 86400000
      topic.creation.heartbeat.include: .*debezium_signal|.*-debezium-.*-heartbeat\..*
      topic.creation.groups: compacted, deleted, dev, heartbeat
      topic.creation.group: default # default group for all connectors
      topic.prefix: ''
      namespace: '' # "topic.prefix" alias, has higher priority
      signal.enabled: false
      signal.enabled.channels: source # [source, kafka]
      signal.data.collection: debezium.debezium_signal # cannot be overridden at an instance level; the same schema should be used as for "heartbeat.action.table"
      plugin.name: pgoutput
      snapshot.mode: initial
      snapshot.delay.ms: 10000
      snapshot.fetch.size: 10240
      max.batch.size: 2048
      max.queue.size: 8192
      poll.interval.ms: 500
      status.update.interval.ms: 10000
      flush.lsn.source: true
      session.timeout.ms: 45000
      heartbeat.interval.ms: 10000
      heartbeat.action.query: '' # See docs/HOW-TO-CONFIGURE.md#notes
      heartbeat.action.table: debezium.debezium_heartbeat # cannot be overridden at an instance level
      topic.heartbeat.prefix: '' # by default: '{{ $.Values.connectorConfig.debezium.instances[*].name }}-heartbeat'
      database.server.name: postgres
      database.hostname: postgres
      database.port: 5432
      database.user: ${env:DEBEZIUM_PG_USER}
      database.password: ${env:DEBEZIUM_PG_PASS}
      database.sslmode: disable
      database.sslrootcert: /mnt/external-configuration/psql-root.crt
      publication.name: '' # by default: connectorConfig.debezium.instances[*].name
      publication.autocreate.mode: filtered # [filtered, disabled]
      slot.name: '' # by default: connectorConfig.debezium.instances[*].name
      slot.max.retries: 6
      slot.retry.delay.ms: 10000
      transforms: RemoveSchemaName
      transforms.RemoveSchemaName.type: org.apache.kafka.connect.transforms.RegexRouter
      transforms.RemoveSchemaName.regex: (.*)\.(public|debezium)\.(.*) # list all the schemas you use
      transforms.RemoveSchemaName.replacement: '$1-connect.__db_name_placeholder__.$3' # database name will be substituted automatically

## It seems to work only with debezium-interceptor: https://mvnrepository.com/artifact/io.debezium/debezium-interceptor
## `producer.interceptor.classes: io.debezium.tracing.DebeziumTracingProducerInterceptor`
## But Strimzi Operator does not allow to set this parameter:
## https://strimzi.io/docs/operators/latest/configuring#type-KafkaConnectSpec-reference
## https://debezium.io/documentation/reference/stable/integrations/tracing.html
## https://github.com/debezium/debezium-examples/blob/main/outbox/debezium-strimzi/Dockerfile
#      transforms.TracingSpan.type: io.debezium.transforms.tracing.ActivateTracingSpan
#      transforms.TracingSpan.tracing.operation.name: debezium-read
#      transforms.TracingSpan.tracing.span.context.field: tracingspancontext
#      transforms.TracingSpan.tracing.with.context.field.only: false
#      transforms.TracingSpan.negate: true
#      transforms.TracingSpan.predicate: debeziumHeartbeat
#      predicates: debeziumHeartbeat
#      predicates.debeziumHeartbeat.type: org.apache.kafka.connect.transforms.predicates.TopicNameMatches
#      predicates.debeziumHeartbeat.pattern: .*debezium_signal|.*-debezium-.*-heartbeat\..*

  separator:
    spec:
      class: org.apache.kafka.connect.mirror.MirrorSourceConnector
## Note: Since the MirrorSourceTask instances share the load over topic partitions,
## there is no point setting the tasksMax property of the connector to higher than
## the number of topic partitions that need to be replicated.
      tasksMax: 6
    instances: []
    config:
      source.cluster.alias: source
      target.cluster.alias: target
      source.cluster.bootstrap.servers: '{{ tpl $.Values.strimziConfig.bootstrapServers $ }}'
      target.cluster.bootstrap.servers: ''
      replication.factor: 3 # Relevant if DefaultReplicationPolicy is used (See description below)
      replication.policy.separator: '-' # Relevant if DefaultReplicationPolicy is used (See description below)
## We use `org.apache.kafka.connect.mirror.IdentityReplicationPolicy` instead of default
## `org.apache.kafka.connect.mirror.DefaultReplicationPolicy` because it adds
## ${source.cluster.alias}${replication.policy.separator} prefix (e.g. `source-`) to the output topic name.
## But since we are using `source` type of connector, exactly this name is used by io.confluent.connect.avro.AvroConverter
## And as a result the avro-schema will not be found as it does not have this prefix
## However, this has certain aspects: first, we will have to add a prefix to the output topic in a different way
## (this is required when using a single Kafka cluster, e.g. for demonstration purposes)
## and second, the topic's parameters will not be replicated (so we will have to take care of this ourselves).
      replication.policy.class: org.apache.kafka.connect.mirror.IdentityReplicationPolicy
## Exclude internal kafka topics from a transformation chain
      predicates: Heartbeats
      predicates.Heartbeats.type: org.apache.kafka.connect.transforms.predicates.TopicNameMatches
      predicates.Heartbeats.pattern: .*heartbeats|.*-debezium-.*-heartbeat\..*|.*debezium_signal
## The chain: Avro > Filter > Json > Prefix
      transforms: ConvertFromBytesKey, ConvertFromBytesValue, Filter, ConvertToBytesValue, ConvertToBytesKey, AddPrefix
## Convert kafka message key from avro format
      transforms.ConvertFromBytesKey.type: com.cloudera.dim.kafka.connect.transformations.convert.ConvertFromBytes$Key
      transforms.ConvertFromBytesKey.converter: io.confluent.connect.avro.AvroConverter
      transforms.ConvertFromBytesKey.converter.schema.registry.url: '{{ tpl $.Values.strimziConfig.schemaRegistry $ }}'
      transforms.ConvertFromBytesKey.converter.schemas.enable: false
      transforms.ConvertFromBytesKey.predicate: Heartbeats
      transforms.ConvertFromBytesKey.negate: true
## Convert kafka message value from avro format
      transforms.ConvertFromBytesValue.type: com.cloudera.dim.kafka.connect.transformations.convert.ConvertFromBytes$Value
      transforms.ConvertFromBytesValue.converter: io.confluent.connect.avro.AvroConverter
      transforms.ConvertFromBytesValue.converter.schema.registry.url: '{{ tpl $.Values.strimziConfig.schemaRegistry $ }}'
      transforms.ConvertFromBytesValue.converter.schemas.enable: false
      transforms.ConvertFromBytesValue.predicate: Heartbeats
      transforms.ConvertFromBytesValue.negate: true
## Message Filtering
      message.filter: '' # Replace with your specific condition
      transforms.Filter.type: io.debezium.transforms.Filter
      transforms.Filter.language: jsr223.groovy
      transforms.Filter.condition: '' # by default: "message.filter", has higher priority
      transforms.Filter.null.handling.mode: keep
      transforms.Filter.predicate: Heartbeats
      transforms.Filter.negate: true
## Convert kafka message key to json format
      transforms.ConvertToBytesValue.type: com.cloudera.dim.kafka.connect.transformations.convert.ConvertToBytes$Value
      transforms.ConvertToBytesValue.converter: org.apache.kafka.connect.json.JsonConverter
      transforms.ConvertToBytesValue.converter.schemas.enable: false # Enable if schema needs to be stored with the payload
      transforms.ConvertToBytesValue.predicate: Heartbeats
      transforms.ConvertToBytesValue.negate: true
## Convert kafka message value to json format
      transforms.ConvertToBytesKey.type: com.cloudera.dim.kafka.connect.transformations.convert.ConvertToBytes$Key
      transforms.ConvertToBytesKey.converter: org.apache.kafka.connect.json.JsonConverter
      transforms.ConvertToBytesKey.converter.schemas.enable: false # Enable if schema needs to be stored with the payload
      transforms.ConvertToBytesKey.predicate: Heartbeats
      transforms.ConvertToBytesKey.negate: true
## Add prefix
      transforms.AddPrefix.type: org.apache.kafka.connect.transforms.RegexRouter
      transforms.AddPrefix.regex: (.*)
      transforms.AddPrefix.replacement: '' # by default: topic.prefix-$1
      transforms.AddPrefix.predicate: Heartbeats
      transforms.AddPrefix.negate: true
## No need for additional converting
      key.converter: org.apache.kafka.connect.converters.ByteArrayConverter
      value.converter: org.apache.kafka.connect.converters.ByteArrayConverter
      header.converter: org.apache.kafka.connect.converters.ByteArrayConverter
## Topic Filtering
      topics: ''
      topics.include: '' # "topics" alias, has higher priority
      topics.exclude: .*heartbeats|.*-debezium-.*-heartbeat\..*|.*debezium_signal
      topic.filter.class: org.apache.kafka.connect.mirror.DefaultTopicFilter
      topic.prefix: '' # by default: "source.cluster.alias" in case of identical source & target bootstrap.servers
## Disable specific sync parameters since we don't use "mirroring"
      refresh.topics.enabled: true
      refresh.topics.interval.seconds: 10
      sync.topic.configs.enabled: true
      sync.topic.configs.interval.seconds: 10
      sync.topic.acls.enabled: false
      emit.offset-syncs.enabled: false
      emit.heartbeats.enabled: false
      emit.checkpoints.enabled: false
      sync.group.offsets.enabled: false

externalConfig:
  rbac:
    enabled: false
    scope: namespaced
    rules: []

  podMonitor:
    enabled: false
    extraLabels: {}
    namespace: '{{ .Release.Namespace }}'
    metricsEndpoints:
      - port: tcp-prometheus
        path: /metrics
        interval: 30s
        scrapeTimeout: 15s
        relabelings:
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(strimzi_io_.+)
            replacement: $1
            separator: ;
          - action: replace
            sourceLabels: [__meta_kubernetes_namespace]
            targetLabel: namespace
            regex: (.*)
            replacement: $1
            separator: ;
          - action: replace
            sourceLabels: [__meta_kubernetes_pod_name]
            targetLabel: kubernetes_pod_name
            regex: (.*)
            replacement: $1
            separator: ;
          - action: replace
            sourceLabels: [__meta_kubernetes_pod_node_name]
            targetLabel: node_name
            regex: (.*)
            replacement: $1
            separator: ;
          - action: replace
            sourceLabels: [__meta_kubernetes_pod_host_ip]
            targetLabel: node_ip
            regex: (.*)
            replacement: $1
            separator: ;

  configMap:
    enabled: false
    name: '{{ .Release.Name }}-external-config'
    mountPath: /mnt/external-configuration # should start with /mnt
    content:
      debezium-bootstrap.sh: '{{ include "debezium.cmd" $ }}'
      psql-root.crt: undefined
      metricsConfig:
      loggingConfig:

  secret:
    enabled: false
    name: '{{ .Release.Name }}-external-secret'
    content:
      postgresUser: postgres
      postgresPassword: postgres

  debeziumConfig:
    secretName: '{{ tpl .Values.externalConfig.secret.name $ }}'
    postgresUsernameKey: postgresUser
    postgresPasswordKey: postgresPassword
    initJob:
      enabled: false

      ttlSecondsAfterFinished: 60

      restartPolicy: Never

      backoffLimit: 1

      image:
        registry: mirror.gcr.io
        repository: library/postgres
        tag: 14-alpine
        imagePullPolicy: IfNotPresent

      command:
        - 'bash'

      args:
        - '-c'
        - 'source /debezium-bootstrap.sh'

      extraLabels: {}

      extraEnv:
        - name: PGHOST
          value: '{{ tpl ( index .Values.connectorConfig.debezium.config "database.hostname" ) $ }}'
        - name: PGPORT
          value: '{{ tpl ( index .Values.connectorConfig.debezium.config "database.port" | toString ) $ }}'
        - name: PGSSLMODE
          value: '{{ tpl ( index .Values.connectorConfig.debezium.config "database.sslmode" ) $ }}'
        - name: PGSSLROOTCERT
          value: /psql-root.crt
        - name: PGUSER
          valueFrom:
            secretKeyRef:
              name: '{{ tpl .Values.externalConfig.debeziumConfig.secretName $ }}'
              key: '{{ tpl .Values.externalConfig.debeziumConfig.postgresUsernameKey $ }}'
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: '{{ tpl .Values.externalConfig.debeziumConfig.secretName $ }}'
              key: '{{ tpl .Values.externalConfig.debeziumConfig.postgresPasswordKey $ }}'

      extraVolumes:
        - name: debezium-config
          configMap:
            name: '{{ tpl .Values.externalConfig.configMap.name $ }}'
            optional: true
            defaultMode: 365

      extraVolumeMounts:
        - name: debezium-config
          mountPath: /psql-root.crt
          subPath: psql-root.crt
          readOnly: true
        - name: debezium-config
          mountPath: /debezium-bootstrap.sh
          subPath: debezium-bootstrap.sh
          readOnly: true

      resources:
        limits:
          cpu: 50m
          memory: 64Mi

      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault

      affinity: {}

      tolerations: []

  extraManifests: []
